{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df8933b9-8101-4964-b195-75f2078a6793",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading vehicles.mp4 assets \n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "635c050cf6b84ed9a3379d3f249e4831",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35345757 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'vehicles.mp4'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from supervision.assets import VideoAssets, download_assets\n",
    "\n",
    "if not os.path.exists(\"data\"):\n",
    "    os.makedirs(\"data\")\n",
    "os.chdir(\"data\")\n",
    "download_assets(VideoAssets.VEHICLES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b327980-72f2-4acb-a9dd-da2761c0f36e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting supervision\n",
      "  Obtaining dependency information for supervision from https://files.pythonhosted.org/packages/c1/24/d3bcad7ece751166ed308c6deb7e7d02a62a7f5a6e01e61ff2787c538fb0/supervision-0.25.1-py3-none-any.whl.metadata\n",
      "  Downloading supervision-0.25.1-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: contourpy>=1.0.7 in c:\\users\\cg lab\\desktop\\project\\face\\lib\\site-packages (from supervision) (1.3.1)\n",
      "Requirement already satisfied: defusedxml<0.8.0,>=0.7.1 in c:\\users\\cg lab\\desktop\\project\\face\\lib\\site-packages (from supervision) (0.7.1)\n",
      "Requirement already satisfied: matplotlib>=3.6.0 in c:\\users\\cg lab\\desktop\\project\\face\\lib\\site-packages (from supervision) (3.10.0)\n",
      "Requirement already satisfied: numpy>=1.21.2 in c:\\users\\cg lab\\desktop\\project\\face\\lib\\site-packages (from supervision) (2.0.2)\n",
      "Collecting opencv-python>=4.5.5.64 (from supervision)\n",
      "  Obtaining dependency information for opencv-python>=4.5.5.64 from https://files.pythonhosted.org/packages/ec/6c/fab8113424af5049f85717e8e527ca3773299a3c6b02506e66436e19874f/opencv_python-4.10.0.84-cp37-abi3-win_amd64.whl.metadata\n",
      "  Downloading opencv_python-4.10.0.84-cp37-abi3-win_amd64.whl.metadata (20 kB)\n",
      "Requirement already satisfied: pillow>=9.4 in c:\\users\\cg lab\\desktop\\project\\face\\lib\\site-packages (from supervision) (11.0.0)\n",
      "Requirement already satisfied: pyyaml>=5.3 in c:\\users\\cg lab\\desktop\\project\\face\\lib\\site-packages (from supervision) (6.0.2)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\cg lab\\desktop\\project\\face\\lib\\site-packages (from supervision) (2.32.3)\n",
      "Requirement already satisfied: scipy<2.0.0,>=1.10.0 in c:\\users\\cg lab\\desktop\\project\\face\\lib\\site-packages (from supervision) (1.14.1)\n",
      "Requirement already satisfied: tqdm>=4.62.3 in c:\\users\\cg lab\\desktop\\project\\face\\lib\\site-packages (from supervision) (4.67.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\cg lab\\desktop\\project\\face\\lib\\site-packages (from matplotlib>=3.6.0->supervision) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\cg lab\\desktop\\project\\face\\lib\\site-packages (from matplotlib>=3.6.0->supervision) (4.55.3)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\cg lab\\desktop\\project\\face\\lib\\site-packages (from matplotlib>=3.6.0->supervision) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\cg lab\\desktop\\project\\face\\lib\\site-packages (from matplotlib>=3.6.0->supervision) (24.2)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\cg lab\\desktop\\project\\face\\lib\\site-packages (from matplotlib>=3.6.0->supervision) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\cg lab\\desktop\\project\\face\\lib\\site-packages (from matplotlib>=3.6.0->supervision) (2.9.0.post0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\cg lab\\desktop\\project\\face\\lib\\site-packages (from requests>=2.26.0->supervision) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\cg lab\\desktop\\project\\face\\lib\\site-packages (from requests>=2.26.0->supervision) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\cg lab\\desktop\\project\\face\\lib\\site-packages (from requests>=2.26.0->supervision) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\cg lab\\desktop\\project\\face\\lib\\site-packages (from requests>=2.26.0->supervision) (2024.8.30)\n",
      "Requirement already satisfied: colorama in c:\\users\\cg lab\\desktop\\project\\face\\lib\\site-packages (from tqdm>=4.62.3->supervision) (0.4.6)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\cg lab\\desktop\\project\\face\\lib\\site-packages (from python-dateutil>=2.7->matplotlib>=3.6.0->supervision) (1.17.0)\n",
      "Downloading supervision-0.25.1-py3-none-any.whl (181 kB)\n",
      "   ---------------------------------------- 0.0/181.5 kB ? eta -:--:--\n",
      "   ------ --------------------------------- 30.7/181.5 kB 1.4 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 112.6/181.5 kB 1.3 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 174.1/181.5 kB 1.3 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 174.1/181.5 kB 1.3 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 174.1/181.5 kB 1.3 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 174.1/181.5 kB 1.3 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 174.1/181.5 kB 1.3 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 174.1/181.5 kB 1.3 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 174.1/181.5 kB 1.3 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 174.1/181.5 kB 1.3 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 174.1/181.5 kB 1.3 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 174.1/181.5 kB 1.3 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 174.1/181.5 kB 1.3 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 174.1/181.5 kB 1.3 MB/s eta 0:00:01\n",
      "   -------------------------------------- 181.5/181.5 kB 274.0 kB/s eta 0:00:00\n",
      "Using cached opencv_python-4.10.0.84-cp37-abi3-win_amd64.whl (38.8 MB)\n",
      "Installing collected packages: opencv-python, supervision\n",
      "Successfully installed opencv-python-4.10.0.84 supervision-0.25.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install supervision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f8866b50-30f6-4e39-8718-4b7999d3aea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load YOLO\n",
    "yolo_net = cv2.dnn.readNet(\"C:/Users/CG Lab/Desktop/project/project1/yolov3.weights\", \"C:/Users/CG Lab/Desktop/project/project1/yolov3.cfg\")\n",
    "layer_names = yolo_net.getLayerNames()\n",
    "output_layers = [layer_names[i - 1] for i in yolo_net.getUnconnectedOutLayers()]\n",
    "\n",
    "# Load COCO class labels\n",
    "with open(\"C:/Users/CG Lab/Desktop/project/project1/coco.names\", \"r\") as f:\n",
    "    classes = [line.strip() for line in f.readlines()]\n",
    "\n",
    "# Initialize video capture (replace 'video.mp4' with your video file)\n",
    "cap = cv2.VideoCapture(\"C:/Users/CG Lab/Desktop/project/project1/video.mp4\")\n",
    "\n",
    "# FPS (Frames per second) from the video\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "# Define tracking parameters\n",
    "tracker = cv2.TrackerMIL_create()  # You can try different trackers like KCF or MIL\n",
    "\n",
    "vehicle_detected = False\n",
    "vehicle_position = None\n",
    "vehicle_speed = 0  # Speed in pixels per second\n",
    "\n",
    "while(cap.isOpened()):\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Get frame dimensions\n",
    "    height, width, channels = frame.shape\n",
    "\n",
    "    # Convert frame to blob for YOLO model input\n",
    "    blob = cv2.dnn.blobFromImage(frame, 0.00392, (416, 416), (0, 0, 0), True, crop=False)\n",
    "    yolo_net.setInput(blob)\n",
    "    outputs = yolo_net.forward(output_layers)\n",
    "\n",
    "    class_ids = []\n",
    "    confidences = []\n",
    "    boxes = []\n",
    "\n",
    "    for out in outputs:\n",
    "        for detection in out:\n",
    "            scores = detection[5:]\n",
    "            class_id = np.argmax(scores)\n",
    "            confidence = scores[class_id]\n",
    "\n",
    "            if confidence > 0.5 and classes[class_id] == 'car':  # Detecting car only\n",
    "                center_x = int(detection[0] * width)\n",
    "                center_y = int(detection[1] * height)\n",
    "                w = int(detection[2] * width)\n",
    "                h = int(detection[3] * height)\n",
    "\n",
    "                # Rectangle coordinates\n",
    "                x = int(center_x - w / 2)\n",
    "                y = int(center_y - h / 2)\n",
    "\n",
    "                boxes.append([x, y, w, h])\n",
    "                confidences.append(float(confidence))\n",
    "                class_ids.append(class_id)\n",
    "\n",
    "    # Apply non-maxima suppression to remove overlapping boxes\n",
    "    indexes = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)\n",
    "\n",
    "    # Start tracking if a vehicle is detected\n",
    "    if len(indexes) > 0:\n",
    "        for i in indexes.flatten():\n",
    "            x, y, w, h = boxes[i]\n",
    "            if not vehicle_detected:\n",
    "                vehicle_position = (x + w // 2, y + h // 2)  # Initial vehicle position\n",
    "                tracker.init(frame, tuple(boxes[i]))\n",
    "                vehicle_detected = True\n",
    "            else:\n",
    "                # Track the vehicle's movement\n",
    "                success, vehicle_box = tracker.update(frame)\n",
    "                if success:\n",
    "                    # Update position\n",
    "                    x, y, w, h = [int(v) for v in vehicle_box]\n",
    "                    new_position = (x + w // 2, y + h // 2)\n",
    "\n",
    "                    # Calculate distance moved in pixels\n",
    "                    distance_moved = np.linalg.norm(np.array(new_position) - np.array(vehicle_position))\n",
    "\n",
    "                    # Calculate speed (distance / time, speed in pixels per second)\n",
    "                    vehicle_speed = distance_moved * fps  # Adjust this for actual speed by converting to meters\n",
    "\n",
    "                    # Update position\n",
    "                    vehicle_position = new_position\n",
    "\n",
    "                    # Draw the bounding box and the speed on the frame\n",
    "                    cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "                    cv2.putText(frame, f\"Speed: {vehicle_speed:.2f} px/s\", (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "    # Show the frame with bounding boxes and speed\n",
    "    cv2.imshow(\"Frame\", frame)\n",
    "\n",
    "    # Press 'q' to quit\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the video capture and close windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "54e9f7a2-0390-46c5-9aef-34b59a1bf22f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-contrib-python in c:\\users\\cg lab\\desktop\\project\\face\\lib\\site-packages (4.10.0.84)\n",
      "Requirement already satisfied: numpy>=1.21.2 in c:\\users\\cg lab\\desktop\\project\\face\\lib\\site-packages (from opencv-contrib-python) (2.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install opencv-contrib-python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "938dca7d-38f2-43ea-b149-e556e2b99deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load YOLO\n",
    "yolo_net = cv2.dnn.readNet(\"C:/Users/CG Lab/Desktop/project/project1/yolov3.weights\", \"C:/Users/CG Lab/Desktop/project/project1/yolov3.cfg\")\n",
    "layer_names = yolo_net.getLayerNames()\n",
    "output_layers = [layer_names[i - 1] for i in yolo_net.getUnconnectedOutLayers()]\n",
    "\n",
    "# Load COCO class labels\n",
    "with open(\"C:/Users/CG Lab/Desktop/project/project1/coco.names\", \"r\") as f:\n",
    "    classes = [line.strip() for line in f.readlines()]\n",
    "\n",
    "# Initialize video capture (replace 'video.mp4' with your video file)\n",
    "cap = cv2.VideoCapture(\"C:/Users/CG Lab/Desktop/project/project1/video_01.mp4\")\n",
    "\n",
    "# FPS (Frames per second) from the video\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "# Define tracking parameters\n",
    "tracker = cv2.TrackerMIL_create()  # You can try different trackers like KCF or MIL\n",
    "\n",
    "vehicle_detected = False\n",
    "vehicle_position = None\n",
    "vehicle_speed = 0  # Speed in pixels per second\n",
    "\n",
    "while(cap.isOpened()):\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Get frame dimensions\n",
    "    height, width, channels = frame.shape\n",
    "\n",
    "    # Convert frame to blob for YOLO model input\n",
    "    blob = cv2.dnn.blobFromImage(frame, 0.00392, (416, 416), (0, 0, 0), True, crop=False)\n",
    "    yolo_net.setInput(blob)\n",
    "    outputs = yolo_net.forward(output_layers)\n",
    "\n",
    "    class_ids = []\n",
    "    confidences = []\n",
    "    boxes = []\n",
    "\n",
    "    for out in outputs:\n",
    "        for detection in out:\n",
    "            scores = detection[5:]\n",
    "            class_id = np.argmax(scores)\n",
    "            confidence = scores[class_id]\n",
    "\n",
    "            if confidence > 0.5 and classes[class_id] == 'car':  # Detecting car only\n",
    "                center_x = int(detection[0] * width)\n",
    "                center_y = int(detection[1] * height)\n",
    "                w = int(detection[2] * width)\n",
    "                h = int(detection[3] * height)\n",
    "\n",
    "                # Rectangle coordinates\n",
    "                x = int(center_x - w / 2)\n",
    "                y = int(center_y - h / 2)\n",
    "\n",
    "                boxes.append([x, y, w, h])\n",
    "                confidences.append(float(confidence))\n",
    "                class_ids.append(class_id)\n",
    "\n",
    "    # Apply non-maxima suppression to remove overlapping boxes\n",
    "    indexes = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)\n",
    "\n",
    "    # Start tracking if a vehicle is detected\n",
    "    if len(indexes) > 0:\n",
    "        for i in indexes.flatten():\n",
    "            x, y, w, h = boxes[i]\n",
    "            if not vehicle_detected:\n",
    "                vehicle_position = (x + w // 2, y + h // 2)  # Initial vehicle position\n",
    "                tracker.init(frame, tuple(boxes[i]))\n",
    "                vehicle_detected = True\n",
    "            else:\n",
    "                # Track the vehicle's movement\n",
    "                success, vehicle_box = tracker.update(frame)\n",
    "                if success:\n",
    "                    # Update position\n",
    "                    x, y, w, h = [int(v) for v in vehicle_box]\n",
    "                    new_position = (x + w // 2, y + h // 2)\n",
    "\n",
    "                    # Calculate distance moved in pixels\n",
    "                    distance_moved = np.linalg.norm(np.array(new_position) - np.array(vehicle_position))\n",
    "\n",
    "                    # Calculate speed (distance / time, speed in pixels per second)\n",
    "                    vehicle_speed = distance_moved * fps  # Adjust this for actual speed by converting to meters\n",
    "\n",
    "                    # Update position\n",
    "                    vehicle_position = new_position\n",
    "\n",
    "                    # Draw the bounding box and the speed on the frame\n",
    "                    cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "                    cv2.putText(frame, f\"Speed: {vehicle_speed:.2f} px/s\", (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "    # Show the frame with bounding boxes and speed\n",
    "    cv2.imshow(\"Frame\", frame)\n",
    "\n",
    "    # Press 'q' to quit\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the video capture and close windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "866d3098-7917-45c3-9993-baf1afb13ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load YOLO\n",
    "yolo_net = cv2.dnn.readNet(\"C:/Users/CG Lab/Desktop/project/project1/yolov3.weights\", \"C:/Users/CG Lab/Desktop/project/project1/yolov3.cfg\")\n",
    "layer_names = yolo_net.getLayerNames()\n",
    "output_layers = [layer_names[i - 1] for i in yolo_net.getUnconnectedOutLayers()]\n",
    "\n",
    "# Load COCO class labels\n",
    "with open(\"C:/Users/CG Lab/Desktop/project/project1/coco.names\", \"r\") as f:\n",
    "    classes = [line.strip() for line in f.readlines()]\n",
    "\n",
    "# Initialize video capture (replace 'video.mp4' with your video file)\n",
    "cap = cv2.VideoCapture(\"C:/Users/CG Lab/Desktop/project/project1/video_01.mp4\")\n",
    "\n",
    "# FPS (Frames per second) from the video\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "# Define tracking parameters\n",
    "tracker = cv2.TrackerMIL_create()  # You can try different trackers like KCF or MIL\n",
    "\n",
    "vehicle_detected = False\n",
    "vehicle_position = None\n",
    "vehicle_speed = 0  # Speed in pixels per second\n",
    "\n",
    "while(cap.isOpened()):\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Resize the frame to speed up processing\n",
    "    frame = cv2.resize(frame, (640, 480))\n",
    "\n",
    "    # Get frame dimensions\n",
    "    height, width, channels = frame.shape\n",
    "\n",
    "    # Convert frame to blob for YOLO model input\n",
    "    blob = cv2.dnn.blobFromImage(frame, 0.00392, (416, 416), (0, 0, 0), True, crop=False)\n",
    "    yolo_net.setInput(blob)\n",
    "    outputs = yolo_net.forward(output_layers)\n",
    "\n",
    "    class_ids = []\n",
    "    confidences = []\n",
    "    boxes = []\n",
    "\n",
    "    for out in outputs:\n",
    "        for detection in out:\n",
    "            scores = detection[5:]\n",
    "            class_id = np.argmax(scores)\n",
    "            confidence = scores[class_id]\n",
    "\n",
    "            if confidence > 0.5 and classes[class_id] == 'car':  # Detecting car only\n",
    "                center_x = int(detection[0] * width)\n",
    "                center_y = int(detection[1] * height)\n",
    "                w = int(detection[2] * width)\n",
    "                h = int(detection[3] * height)\n",
    "\n",
    "                # Rectangle coordinates\n",
    "                x = int(center_x - w / 2)\n",
    "                y = int(center_y - h / 2)\n",
    "\n",
    "                boxes.append([x, y, w, h])\n",
    "                confidences.append(float(confidence))\n",
    "                class_ids.append(class_id)\n",
    "\n",
    "    # Apply non-maxima suppression to remove overlapping boxes\n",
    "    indexes = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)\n",
    "\n",
    "    # Start tracking if a vehicle is detected\n",
    "    if len(indexes) > 0:\n",
    "        for i in indexes.flatten():\n",
    "            x, y, w, h = boxes[i]\n",
    "            if not vehicle_detected:\n",
    "                vehicle_position = (x + w // 2, y + h // 2)  # Initial vehicle position\n",
    "                tracker.init(frame, tuple(boxes[i]))\n",
    "                vehicle_detected = True\n",
    "            else:\n",
    "                # Track the vehicle's movement\n",
    "                success, vehicle_box = tracker.update(frame)\n",
    "                if success:\n",
    "                    # Update position\n",
    "                    x, y, w, h = [int(v) for v in vehicle_box]\n",
    "                    new_position = (x + w // 2, y + h // 2)\n",
    "\n",
    "                    # Calculate distance moved in pixels\n",
    "                    distance_moved = np.linalg.norm(np.array(new_position) - np.array(vehicle_position))\n",
    "\n",
    "                    # Calculate speed (distance / time, speed in pixels per second)\n",
    "                    vehicle_speed = distance_moved * fps * 10  # Scaling factor for better visibility\n",
    "\n",
    "                    # Update position\n",
    "                    vehicle_position = new_position\n",
    "\n",
    "                    # Draw the bounding box and the speed on the frame\n",
    "                    cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 3)  # Thicker bounding box\n",
    "                    cv2.putText(frame, f\"Speed: {vehicle_speed:.2f} px/s\", (x, y - 20), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "    # Show the frame with bounding boxes and speed\n",
    "    cv2.imshow(\"Frame\", frame)\n",
    "\n",
    "    # Press 'q' to quit\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the video capture and close windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "95f74f7a-87a0-4302-802f-31cf56979151",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load YOLO\n",
    "yolo_net = cv2.dnn.readNet(\"C:/Users/CG Lab/Desktop/project/project1/yolov3.weights\", \"C:/Users/CG Lab/Desktop/project/project1/yolov3.cfg\")\n",
    "layer_names = yolo_net.getLayerNames()\n",
    "output_layers = [layer_names[i - 1] for i in yolo_net.getUnconnectedOutLayers()]\n",
    "\n",
    "# Load COCO class labels\n",
    "with open(\"C:/Users/CG Lab/Desktop/project/project1/coco.names\", \"r\") as f:\n",
    "    classes = [line.strip() for line in f.readlines()]\n",
    "\n",
    "# Initialize video capture (replace 'video.mp4' with your video file)\n",
    "cap = cv2.VideoCapture(\"C:/Users/CG Lab/Desktop/project/project1/video_01.mp4\")\n",
    "\n",
    "# FPS (Frames per second) from the video\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "# List to store trackers for each car\n",
    "trackers = []\n",
    "vehicle_positions = []  # Store the initial positions of each vehicle\n",
    "vehicle_speeds = []     # Store the speed of each vehicle\n",
    "\n",
    "while(cap.isOpened()):\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Resize the frame to speed up processing\n",
    "    frame = cv2.resize(frame, (640, 480))\n",
    "\n",
    "    # Get frame dimensions\n",
    "    height, width, channels = frame.shape\n",
    "\n",
    "    # Convert frame to blob for YOLO model input\n",
    "    blob = cv2.dnn.blobFromImage(frame, 0.00392, (416, 416), (0, 0, 0), True, crop=False)\n",
    "    yolo_net.setInput(blob)\n",
    "    outputs = yolo_net.forward(output_layers)\n",
    "\n",
    "    class_ids = []\n",
    "    confidences = []\n",
    "    boxes = []\n",
    "\n",
    "    for out in outputs:\n",
    "        for detection in out:\n",
    "            scores = detection[5:]\n",
    "            class_id = np.argmax(scores)\n",
    "            confidence = scores[class_id]\n",
    "\n",
    "            if confidence > 0.5 and classes[class_id] == 'car':  # Detecting car only\n",
    "                center_x = int(detection[0] * width)\n",
    "                center_y = int(detection[1] * height)\n",
    "                w = int(detection[2] * width)\n",
    "                h = int(detection[3] * height)\n",
    "\n",
    "                # Rectangle coordinates\n",
    "                x = int(center_x - w / 2)\n",
    "                y = int(center_y - h / 2)\n",
    "\n",
    "                boxes.append([x, y, w, h])\n",
    "                confidences.append(float(confidence))\n",
    "                class_ids.append(class_id)\n",
    "\n",
    "    # Apply non-maxima suppression to remove overlapping boxes\n",
    "    indexes = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)\n",
    "\n",
    "    # Initialize trackers for all detected cars\n",
    "    if len(indexes) > 0:\n",
    "        for i in indexes.flatten():\n",
    "            x, y, w, h = boxes[i]\n",
    "            # Create a tracker for each detected car and initialize it\n",
    "            tracker = cv2.TrackerMIL_create()\n",
    "            trackers.append(tracker)\n",
    "            vehicle_positions.append((x + w // 2, y + h // 2))  # Store initial position for speed calculation\n",
    "            tracker.init(frame, tuple(boxes[i]))  # Initialize tracker for each car\n",
    "\n",
    "    # Track all vehicles\n",
    "    for i, tracker in enumerate(trackers):\n",
    "        success, vehicle_box = tracker.update(frame)\n",
    "        if success:\n",
    "            # Update position for each vehicle\n",
    "            x, y, w, h = [int(v) for v in vehicle_box]\n",
    "            new_position = (x + w // 2, y + h // 2)\n",
    "\n",
    "            # Calculate the distance moved for the current vehicle\n",
    "            distance_moved = np.linalg.norm(np.array(new_position) - np.array(vehicle_positions[i]))\n",
    "\n",
    "            # Calculate speed in pixels per second\n",
    "            vehicle_speed = distance_moved * fps * 10  # Scaling factor to exaggerate speed\n",
    "\n",
    "            # Update position\n",
    "            vehicle_positions[i] = new_position\n",
    "\n",
    "            # Draw bounding box and speed for the current vehicle\n",
    "            cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 3)  # Thicker bounding box\n",
    "            cv2.putText(frame, f\"Speed: {vehicle_speed:.2f} px/s\", (x, y - 20), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "    # Show the frame with bounding boxes and speed\n",
    "    cv2.imshow(\"Frame\", frame)\n",
    "\n",
    "    # Press 'q' to quit\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the video capture and close windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1e8aabcf-cd79-417b-9d0b-ac971c441838",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load YOLO\n",
    "yolo_net = cv2.dnn.readNet(\"C:/Users/CG Lab/Desktop/project/project1/yolov3.weights\", \"C:/Users/CG Lab/Desktop/project/project1/yolov3.cfg\")\n",
    "layer_names = yolo_net.getLayerNames()\n",
    "output_layers = [layer_names[i - 1] for i in yolo_net.getUnconnectedOutLayers()]\n",
    "\n",
    "# Load COCO class labels\n",
    "with open(\"C:/Users/CG Lab/Desktop/project/project1/coco.names\", \"r\") as f:\n",
    "    classes = [line.strip() for line in f.readlines()]\n",
    "\n",
    "# Initialize video capture (replace 'video.mp4' with your video file)\n",
    "cap = cv2.VideoCapture(\"C:/Users/CG Lab/Desktop/project/project1/video_01.mp4\")\n",
    "\n",
    "# FPS (Frames per second) from the video\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "# List to store trackers for each car\n",
    "trackers = []\n",
    "vehicle_positions = []  # Store the initial positions of each vehicle\n",
    "vehicle_speeds = []     # Store the speed of each vehicle\n",
    "frame_count = 0\n",
    "frame_interval = 10  # Run detection every N frames\n",
    "\n",
    "while(cap.isOpened()):\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    frame_count += 1\n",
    "\n",
    "    # Resize the frame to speed up processing\n",
    "    frame = cv2.resize(frame, (640, 480))\n",
    "\n",
    "    # Get frame dimensions\n",
    "    height, width, channels = frame.shape\n",
    "\n",
    "    # Convert frame to blob for YOLO model input\n",
    "    blob = cv2.dnn.blobFromImage(frame, 0.00392, (416, 416), (0, 0, 0), True, crop=False)\n",
    "    yolo_net.setInput(blob)\n",
    "    outputs = yolo_net.forward(output_layers)\n",
    "\n",
    "    class_ids = []\n",
    "    confidences = []\n",
    "    boxes = []\n",
    "\n",
    "    if frame_count % frame_interval == 0:\n",
    "        # Detect vehicles every 'frame_interval' frames\n",
    "        for out in outputs:\n",
    "            for detection in out:\n",
    "                scores = detection[5:]\n",
    "                class_id = np.argmax(scores)\n",
    "                confidence = scores[class_id]\n",
    "\n",
    "                if confidence > 0.5 and classes[class_id] == 'car':  # Detecting car only\n",
    "                    center_x = int(detection[0] * width)\n",
    "                    center_y = int(detection[1] * height)\n",
    "                    w = int(detection[2] * width)\n",
    "                    h = int(detection[3] * height)\n",
    "\n",
    "                    # Rectangle coordinates\n",
    "                    x = int(center_x - w / 2)\n",
    "                    y = int(center_y - h / 2)\n",
    "\n",
    "                    boxes.append([x, y, w, h])\n",
    "                    confidences.append(float(confidence))\n",
    "                    class_ids.append(class_id)\n",
    "\n",
    "        # Apply non-maxima suppression to remove overlapping boxes\n",
    "        indexes = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)\n",
    "\n",
    "        if len(indexes) > 0:\n",
    "            for i in indexes.flatten():\n",
    "                x, y, w, h = boxes[i]\n",
    "                # Check if the car is already being tracked\n",
    "                vehicle_detected = False\n",
    "                for tracker in trackers:\n",
    "                    # Check if a tracker is already tracking this car\n",
    "                    success, box = tracker.update(frame)\n",
    "                    if success:\n",
    "                        vehicle_detected = True\n",
    "                        break\n",
    "\n",
    "                if not vehicle_detected:\n",
    "                    # Create a tracker for each new car detected\n",
    "                    tracker = cv2.TrackerMIL_create()\n",
    "                    trackers.append(tracker)\n",
    "                    vehicle_positions.append((x + w // 2, y + h // 2))  # Store initial position for speed calculation\n",
    "                    tracker.init(frame, tuple(boxes[i]))  # Initialize tracker for each car\n",
    "\n",
    "    # Track all vehicles\n",
    "    for i, tracker in enumerate(trackers):\n",
    "        success, vehicle_box = tracker.update(frame)\n",
    "        if success:\n",
    "            # Update position for each vehicle\n",
    "            x, y, w, h = [int(v) for v in vehicle_box]\n",
    "            new_position = (x + w // 2, y + h // 2)\n",
    "\n",
    "            # Calculate the distance moved for the current vehicle\n",
    "            distance_moved = np.linalg.norm(np.array(new_position) - np.array(vehicle_positions[i]))\n",
    "\n",
    "            # Calculate speed in pixels per second\n",
    "            vehicle_speed = distance_moved * fps * 10  # Scaling factor to exaggerate speed\n",
    "\n",
    "            # Update position\n",
    "            vehicle_positions[i] = new_position\n",
    "\n",
    "            # Draw bounding box and speed for the current vehicle\n",
    "            cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 3)  # Thicker bounding box\n",
    "            cv2.putText(frame, f\"Speed: {vehicle_speed:.2f} px/s\", (x, y - 20), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "    # Show the frame with bounding boxes and speed\n",
    "    cv2.imshow(\"Frame\", frame)\n",
    "\n",
    "    # Press 'q' to quit\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the video capture and close windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ff298990-31f4-4f78-93c3-9307c52db6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Load YOLO\n",
    "yolo_net = cv2.dnn.readNet(\"C:/Users/CG Lab/Desktop/project/project1/yolov3.weights\", \"C:/Users/CG Lab/Desktop/project/project1/yolov3.cfg\")\n",
    "layer_names = yolo_net.getLayerNames()\n",
    "output_layers = [layer_names[i - 1] for i in yolo_net.getUnconnectedOutLayers()]\n",
    "\n",
    "# Load COCO class labels\n",
    "with open(\"C:/Users/CG Lab/Desktop/project/project1/coco.names\", \"r\") as f:\n",
    "    classes = [line.strip() for line in f.readlines()]\n",
    "\n",
    "# Initialize video capture (replace 'video.mp4' with your video file)\n",
    "cap = cv2.VideoCapture(\"C:/Users/CG Lab/Desktop/project/project1/video_01.mp4\")\n",
    "\n",
    "# FPS (Frames per second) from the video\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "# List to store trackers for each car\n",
    "trackers = []\n",
    "vehicle_positions = []  # Store the initial positions of each vehicle\n",
    "vehicle_speeds = []     # Store the speed of each vehicle\n",
    "frame_count = 0\n",
    "frame_interval = 10  # Run detection every N frames\n",
    "vehicle_colors = []  # List to store unique colors for each vehicle\n",
    "\n",
    "def random_color():\n",
    "    \"\"\"Generate a random color for each car.\"\"\"\n",
    "    return [random.randint(0, 255), random.randint(0, 255), random.randint(0, 255)]\n",
    "\n",
    "while(cap.isOpened()):\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    frame_count += 1\n",
    "\n",
    "    # Resize the frame to speed up processing\n",
    "    frame = cv2.resize(frame, (640, 480))\n",
    "\n",
    "    # Get frame dimensions\n",
    "    height, width, channels = frame.shape\n",
    "\n",
    "    # Convert frame to blob for YOLO model input\n",
    "    blob = cv2.dnn.blobFromImage(frame, 0.00392, (416, 416), (0, 0, 0), True, crop=False)\n",
    "    yolo_net.setInput(blob)\n",
    "    outputs = yolo_net.forward(output_layers)\n",
    "\n",
    "    class_ids = []\n",
    "    confidences = []\n",
    "    boxes = []\n",
    "\n",
    "    if frame_count % frame_interval == 0:\n",
    "        # Detect vehicles every 'frame_interval' frames\n",
    "        for out in outputs:\n",
    "            for detection in out:\n",
    "                scores = detection[5:]\n",
    "                class_id = np.argmax(scores)\n",
    "                confidence = scores[class_id]\n",
    "\n",
    "                if confidence > 0.5 and classes[class_id] == 'car':  # Detecting car only\n",
    "                    center_x = int(detection[0] * width)\n",
    "                    center_y = int(detection[1] * height)\n",
    "                    w = int(detection[2] * width)\n",
    "                    h = int(detection[3] * height)\n",
    "\n",
    "                    # Rectangle coordinates\n",
    "                    x = int(center_x - w / 2)\n",
    "                    y = int(center_y - h / 2)\n",
    "\n",
    "                    boxes.append([x, y, w, h])\n",
    "                    confidences.append(float(confidence))\n",
    "                    class_ids.append(class_id)\n",
    "\n",
    "        # Apply non-maxima suppression to remove overlapping boxes\n",
    "        indexes = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)\n",
    "\n",
    "        if len(indexes) > 0:\n",
    "            for i in indexes.flatten():\n",
    "                x, y, w, h = boxes[i]\n",
    "\n",
    "                # Check if the car is already being tracked\n",
    "                vehicle_detected = False\n",
    "                for tracker in trackers:\n",
    "                    # Check if a tracker is already tracking this car\n",
    "                    success, box = tracker.update(frame)\n",
    "                    if success:\n",
    "                        vehicle_detected = True\n",
    "                        break\n",
    "\n",
    "                if not vehicle_detected:\n",
    "                    # Create a tracker for each new car detected\n",
    "                    tracker = cv2.TrackerMIL_create()\n",
    "                    trackers.append(tracker)\n",
    "                    vehicle_positions.append((x + w // 2, y + h // 2))  # Store initial position for speed calculation\n",
    "                    vehicle_colors.append(random_color())  # Assign a random color for this vehicle\n",
    "                    tracker.init(frame, tuple(boxes[i]))  # Initialize tracker for each car\n",
    "\n",
    "    # Track all vehicles\n",
    "    for i, tracker in enumerate(trackers):\n",
    "        success, vehicle_box = tracker.update(frame)\n",
    "        if success:\n",
    "            # Update position for each vehicle\n",
    "            x, y, w, h = [int(v) for v in vehicle_box]\n",
    "            new_position = (x + w // 2, y + h // 2)\n",
    "\n",
    "            # Calculate the distance moved for the current vehicle\n",
    "            distance_moved = np.linalg.norm(np.array(new_position) - np.array(vehicle_positions[i]))\n",
    "\n",
    "            # Calculate speed in pixels per second\n",
    "            vehicle_speed = distance_moved * fps * 10  # Scaling factor to exaggerate speed\n",
    "\n",
    "            # Update position\n",
    "            vehicle_positions[i] = new_position\n",
    "\n",
    "            # Draw bounding box and speed for the current vehicle with its unique color\n",
    "            cv2.rectangle(frame, (x, y), (x + w, y + h), vehicle_colors[i], 3)  # Bounding box with unique color\n",
    "            cv2.putText(frame, f\"Speed: {vehicle_speed:.2f} px/s\", (x, y - 20), cv2.FONT_HERSHEY_SIMPLEX, 1, vehicle_colors[i], 2)\n",
    "\n",
    "    # Show the frame with bounding boxes and speed\n",
    "    cv2.imshow(\"Frame\", frame)\n",
    "\n",
    "    # Press 'q' to quit\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the video capture and close windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "57838225-201f-4942-bd3d-80905c82fedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Load YOLO\n",
    "yolo_net = cv2.dnn.readNet(\"C:/Users/CG Lab/Desktop/project/project1/yolov3.weights\", \"C:/Users/CG Lab/Desktop/project/project1/yolov3.cfg\")\n",
    "layer_names = yolo_net.getLayerNames()\n",
    "output_layers = [layer_names[i - 1] for i in yolo_net.getUnconnectedOutLayers()]\n",
    "\n",
    "# Load COCO class labels\n",
    "with open(\"C:/Users/CG Lab/Desktop/project/project1/coco.names\", \"r\") as f:\n",
    "    classes = [line.strip() for line in f.readlines()]\n",
    "\n",
    "# Initialize video capture (replace 'video.mp4' with your video file)\n",
    "cap = cv2.VideoCapture(\"C:/Users/CG Lab/Desktop/project/project1/video.mp4\")\n",
    "\n",
    "# FPS (Frames per second) from the video\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "# List to store trackers for each car\n",
    "trackers = []\n",
    "vehicle_positions = []  # Store the initial positions of each vehicle\n",
    "vehicle_speeds = []     # Store the speed of each vehicle\n",
    "frame_count = 0\n",
    "frame_interval = 10  # Run detection every N frames\n",
    "vehicle_colors = []  # List to store unique colors for each vehicle\n",
    "\n",
    "def random_color():\n",
    "    \"\"\"Generate a random color for each car.\"\"\"\n",
    "    return [random.randint(0, 255), random.randint(0, 255), random.randint(0, 255)]\n",
    "\n",
    "while(cap.isOpened()):\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    frame_count += 1\n",
    "\n",
    "    # Resize the frame to speed up processing\n",
    "    frame = cv2.resize(frame, (640, 480))\n",
    "\n",
    "    # Get frame dimensions\n",
    "    height, width, channels = frame.shape\n",
    "\n",
    "    # Convert frame to blob for YOLO model input\n",
    "    blob = cv2.dnn.blobFromImage(frame, 0.00392, (416, 416), (0, 0, 0), True, crop=False)\n",
    "    yolo_net.setInput(blob)\n",
    "    outputs = yolo_net.forward(output_layers)\n",
    "\n",
    "    class_ids = []\n",
    "    confidences = []\n",
    "    boxes = []\n",
    "\n",
    "    if frame_count % frame_interval == 0:\n",
    "        # Detect vehicles every 'frame_interval' frames\n",
    "        for out in outputs:\n",
    "            for detection in out:\n",
    "                scores = detection[5:]\n",
    "                class_id = np.argmax(scores)\n",
    "                confidence = scores[class_id]\n",
    "\n",
    "                if confidence > 0.5 and classes[class_id] == 'car':  # Detecting car only\n",
    "                    center_x = int(detection[0] * width)\n",
    "                    center_y = int(detection[1] * height)\n",
    "                    w = int(detection[2] * width)\n",
    "                    h = int(detection[3] * height)\n",
    "\n",
    "                    # Rectangle coordinates\n",
    "                    x = int(center_x - w / 2)\n",
    "                    y = int(center_y - h / 2)\n",
    "\n",
    "                    boxes.append([x, y, w, h])\n",
    "                    confidences.append(float(confidence))\n",
    "                    class_ids.append(class_id)\n",
    "\n",
    "        # Apply non-maxima suppression to remove overlapping boxes\n",
    "        indexes = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)\n",
    "\n",
    "        if len(indexes) > 0:\n",
    "            for i in indexes.flatten():\n",
    "                x, y, w, h = boxes[i]\n",
    "\n",
    "                # Check if the car is already being tracked\n",
    "                vehicle_detected = False\n",
    "                for tracker in trackers:\n",
    "                    # Check if a tracker is already tracking this car\n",
    "                    success, box = tracker.update(frame)\n",
    "                    if success:\n",
    "                        vehicle_detected = True\n",
    "                        break\n",
    "\n",
    "                if not vehicle_detected:\n",
    "                    # Create a tracker for each new car detected\n",
    "                    tracker = cv2.TrackerMIL_create()\n",
    "                    trackers.append(tracker)\n",
    "                    vehicle_positions.append((x + w // 2, y + h // 2))  # Store initial position for speed calculation\n",
    "                    vehicle_colors.append(random_color())  # Assign a random color for this vehicle\n",
    "                    tracker.init(frame, tuple(boxes[i]))  # Initialize tracker for each car\n",
    "\n",
    "    # Track all vehicles\n",
    "    for i, tracker in enumerate(trackers):\n",
    "        success, vehicle_box = tracker.update(frame)\n",
    "        if success:\n",
    "            # Update position for each vehicle\n",
    "            x, y, w, h = [int(v) for v in vehicle_box]\n",
    "            new_position = (x + w // 2, y + h // 2)\n",
    "\n",
    "            # Calculate the distance moved for the current vehicle\n",
    "            distance_moved = np.linalg.norm(np.array(new_position) - np.array(vehicle_positions[i]))\n",
    "\n",
    "            # Calculate speed in pixels per second\n",
    "            vehicle_speed = distance_moved * fps * 10  # Scaling factor to exaggerate speed\n",
    "\n",
    "            # Update position\n",
    "            vehicle_positions[i] = new_position\n",
    "\n",
    "            # Draw bounding box and speed for the current vehicle with its unique color\n",
    "            cv2.rectangle(frame, (x, y), (x + w, y + h), vehicle_colors[i], 3)  # Bounding box with unique color\n",
    "            cv2.putText(frame, f\"Speed: {vehicle_speed:.2f} px/s\", (x, y - 20), cv2.FONT_HERSHEY_SIMPLEX, 1, vehicle_colors[i], 2)\n",
    "\n",
    "    # Show the frame with bounding boxes and speed\n",
    "    cv2.imshow(\"Frame\", frame)\n",
    "\n",
    "    # Press 'q' to quit\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the video capture and close windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "45765399-4e08-46f9-9767-4eef2da630b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load YOLO\n",
    "yolo_net = cv2.dnn.readNet(\"C:/Users/CG Lab/Desktop/project/project1/yolov3.weights\", \"C:/Users/CG Lab/Desktop/project/project1/yolov3.cfg\")\n",
    "layer_names = yolo_net.getLayerNames()\n",
    "output_layers = [layer_names[i - 1] for i in yolo_net.getUnconnectedOutLayers()]\n",
    "\n",
    "# Load COCO class labels\n",
    "with open(\"C:/Users/CG Lab/Desktop/project/project1/coco.names\", \"r\") as f:\n",
    "    classes = [line.strip() for line in f.readlines()]\n",
    "\n",
    "# Initialize video capture (replace 'video.mp4' with your video file)\n",
    "cap = cv2.VideoCapture(\"C:/Users/CG Lab/Desktop/project/project1/video.mp4\")\n",
    "\n",
    "# FPS (Frames per second) from the video\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "# List to store detected boxes and corresponding class IDs\n",
    "frame_count = 0\n",
    "frame_interval = 10  # Run detection every N frames\n",
    "\n",
    "while(cap.isOpened()):\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    frame_count += 1\n",
    "\n",
    "    # Resize the frame to speed up processing\n",
    "    frame = cv2.resize(frame, (640, 480))\n",
    "\n",
    "    # Get frame dimensions\n",
    "    height, width, channels = frame.shape\n",
    "\n",
    "    # Convert frame to blob for YOLO model input\n",
    "    blob = cv2.dnn.blobFromImage(frame, 0.00392, (416, 416), (0, 0, 0), True, crop=False)\n",
    "    yolo_net.setInput(blob)\n",
    "    outputs = yolo_net.forward(output_layers)\n",
    "\n",
    "    class_ids = []\n",
    "    confidences = []\n",
    "    boxes = []\n",
    "\n",
    "    if frame_count % frame_interval == 0:\n",
    "        # Detect vehicles every 'frame_interval' frames\n",
    "        for out in outputs:\n",
    "            for detection in out:\n",
    "                scores = detection[5:]\n",
    "                class_id = np.argmax(scores)\n",
    "                confidence = scores[class_id]\n",
    "\n",
    "                if confidence > 0.5 and (classes[class_id] == 'car' or classes[class_id] == 'truck'):  # Detecting car or truck\n",
    "                    center_x = int(detection[0] * width)\n",
    "                    center_y = int(detection[1] * height)\n",
    "                    w = int(detection[2] * width)\n",
    "                    h = int(detection[3] * height)\n",
    "\n",
    "                    # Rectangle coordinates\n",
    "                    x = int(center_x - w / 2)\n",
    "                    y = int(center_y - h / 2)\n",
    "\n",
    "                    boxes.append([x, y, w, h])\n",
    "                    confidences.append(float(confidence))\n",
    "                    class_ids.append(class_id)\n",
    "\n",
    "        # Apply non-maxima suppression to remove overlapping boxes\n",
    "        indexes = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)\n",
    "\n",
    "        if len(indexes) > 0:\n",
    "            for i in indexes.flatten():\n",
    "                x, y, w, h = boxes[i]\n",
    "                label = str(classes[class_ids[i]])\n",
    "                confidence = confidences[i]\n",
    "\n",
    "                # Draw bounding box and label for the current vehicle (car or truck)\n",
    "                color = (0, 255, 0) if label == 'car' else (0, 0, 255)  # Green for car, Red for truck\n",
    "                cv2.rectangle(frame, (x, y), (x + w, y + h), color, 3)\n",
    "                cv2.putText(frame, f\"{label} ({confidence*100:.2f}%)\", (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 1, color, 2)\n",
    "\n",
    "    # Show the frame with bounding boxes\n",
    "    cv2.imshow(\"Frame\", frame)\n",
    "\n",
    "    # Press 'q' to quit\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the video capture and close windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bcfd1750-85f6-47d0-9e96-c4c582023163",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 96\u001b[0m\n\u001b[0;32m     93\u001b[0m         x, y, w, h \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mint\u001b[39m(v) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m vehicle_box]\n\u001b[0;32m     95\u001b[0m         \u001b[38;5;66;03m# Draw the bounding box for the tracked vehicle\u001b[39;00m\n\u001b[1;32m---> 96\u001b[0m         color \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m255\u001b[39m, \u001b[38;5;241m0\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m classes[\u001b[43mclass_ids\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcar\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m255\u001b[39m)  \u001b[38;5;66;03m# Green for car, Red for truck\u001b[39;00m\n\u001b[0;32m     97\u001b[0m         cv2\u001b[38;5;241m.\u001b[39mrectangle(frame, (x, y), (x \u001b[38;5;241m+\u001b[39m w, y \u001b[38;5;241m+\u001b[39m h), color, \u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m     99\u001b[0m \u001b[38;5;66;03m# Show the frame with bounding boxes\u001b[39;00m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load YOLO model\n",
    "yolo_net = cv2.dnn.readNet(\"C:/Users/CG Lab/Desktop/project/project1/yolov3.weights\", \"C:/Users/CG Lab/Desktop/project/project1/yolov3.cfg\")\n",
    "layer_names = yolo_net.getLayerNames()\n",
    "output_layers = [layer_names[i - 1] for i in yolo_net.getUnconnectedOutLayers()]\n",
    "\n",
    "# Load COCO class labels\n",
    "with open(\"C:/Users/CG Lab/Desktop/project/project1/coco.names\", \"r\") as f:\n",
    "    classes = [line.strip() for line in f.readlines()]\n",
    "\n",
    "# Initialize video capture (replace 'video.mp4' with your video file)\n",
    "cap = cv2.VideoCapture(\"C:/Users/CG Lab/Desktop/project/project1/video.mp4\")\n",
    "\n",
    "# FPS (Frames per second) from the video\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "# List to store detected boxes and trackers\n",
    "trackers = []\n",
    "vehicle_positions = []  # Store the initial positions of each vehicle\n",
    "frame_count = 0\n",
    "frame_interval = 10  # Run detection every N frames\n",
    "\n",
    "while(cap.isOpened()):\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    frame_count += 1\n",
    "\n",
    "    # Resize the frame to speed up processing\n",
    "    frame = cv2.resize(frame, (640, 480))\n",
    "\n",
    "    # Get frame dimensions\n",
    "    height, width, channels = frame.shape\n",
    "\n",
    "    # Convert frame to blob for YOLO model input\n",
    "    blob = cv2.dnn.blobFromImage(frame, 0.00392, (416, 416), (0, 0, 0), True, crop=False)\n",
    "    yolo_net.setInput(blob)\n",
    "    outputs = yolo_net.forward(output_layers)\n",
    "\n",
    "    class_ids = []\n",
    "    confidences = []\n",
    "    boxes = []\n",
    "\n",
    "    if frame_count % frame_interval == 0:\n",
    "        # Detect vehicles every 'frame_interval' frames\n",
    "        for out in outputs:\n",
    "            for detection in out:\n",
    "                scores = detection[5:]\n",
    "                class_id = np.argmax(scores)\n",
    "                confidence = scores[class_id]\n",
    "\n",
    "                if confidence > 0.5 and (classes[class_id] == 'car' or classes[class_id] == 'truck'):  # Detecting car or truck\n",
    "                    center_x = int(detection[0] * width)\n",
    "                    center_y = int(detection[1] * height)\n",
    "                    w = int(detection[2] * width)\n",
    "                    h = int(detection[3] * height)\n",
    "\n",
    "                    # Rectangle coordinates\n",
    "                    x = int(center_x - w / 2)\n",
    "                    y = int(center_y - h / 2)\n",
    "\n",
    "                    boxes.append([x, y, w, h])\n",
    "                    confidences.append(float(confidence))\n",
    "                    class_ids.append(class_id)\n",
    "\n",
    "        # Apply non-maxima suppression to remove overlapping boxes\n",
    "        indexes = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)\n",
    "\n",
    "        # Create a tracker for each detected vehicle\n",
    "        if len(indexes) > 0:\n",
    "            for i in indexes.flatten():\n",
    "                x, y, w, h = boxes[i]\n",
    "                label = str(classes[class_ids[i]])\n",
    "                confidence = confidences[i]\n",
    "\n",
    "                # Initialize tracker for each detected car or truck\n",
    "                tracker = cv2.TrackerMIL_create()\n",
    "                trackers.append(tracker)\n",
    "                tracker.init(frame, tuple(boxes[i]))\n",
    "\n",
    "                # Draw bounding box and label for the current vehicle\n",
    "                color = (0, 255, 0) if label == 'car' else (0, 0, 255)  # Green for car, Red for truck\n",
    "                cv2.rectangle(frame, (x, y), (x + w, y + h), color, 3)\n",
    "                cv2.putText(frame, f\"{label} ({confidence*100:.2f}%)\", (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 1, color, 2)\n",
    "\n",
    "    # Track vehicles across frames\n",
    "    for tracker in trackers:\n",
    "        success, vehicle_box = tracker.update(frame)\n",
    "        if success:\n",
    "            x, y, w, h = [int(v) for v in vehicle_box]\n",
    "\n",
    "            # Draw the bounding box for the tracked vehicle\n",
    "            color = (0, 255, 0) if classes[class_ids[i]] == 'car' else (0, 0, 255)  # Green for car, Red for truck\n",
    "            cv2.rectangle(frame, (x, y), (x + w, y + h), color, 3)\n",
    "\n",
    "    # Show the frame with bounding boxes\n",
    "    cv2.imshow(\"Frame\", frame)\n",
    "\n",
    "    # Press 'q' to quit\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the video capture and close windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3ad2ea6-c339-4458-856c-ba9506724dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load YOLO model\n",
    "yolo_net = cv2.dnn.readNet(\"C:/Users/CG Lab/Desktop/project/project1/yolov3.weights\", \"C:/Users/CG Lab/Desktop/project/project1/yolov3.cfg\")\n",
    "layer_names = yolo_net.getLayerNames()\n",
    "output_layers = [layer_names[i - 1] for i in yolo_net.getUnconnectedOutLayers()]\n",
    "\n",
    "# Load COCO class labels\n",
    "with open(\"C:/Users/CG Lab/Desktop/project/project1/coco.names\", \"r\") as f:\n",
    "    classes = [line.strip() for line in f.readlines()]\n",
    "\n",
    "# Initialize video capture (replace 'video.mp4' with your video file)\n",
    "cap = cv2.VideoCapture(\"C:/Users/CG Lab/Desktop/project/project1/video.mp4\")\n",
    "\n",
    "# FPS (Frames per second) from the video\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "# List to store detected boxes, trackers, and their associated class ids\n",
    "trackers = []\n",
    "vehicle_info = []  # To store vehicle-related information (e.g., class, color)\n",
    "\n",
    "frame_count = 0\n",
    "frame_interval = 10  # Run detection every N frames\n",
    "\n",
    "while(cap.isOpened()):\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    frame_count += 1\n",
    "\n",
    "    # Resize the frame to speed up processing\n",
    "    frame = cv2.resize(frame, (640, 480))\n",
    "\n",
    "    # Get frame dimensions\n",
    "    height, width, channels = frame.shape\n",
    "\n",
    "    # Convert frame to blob for YOLO model input\n",
    "    blob = cv2.dnn.blobFromImage(frame, 0.00392, (416, 416), (0, 0, 0), True, crop=False)\n",
    "    yolo_net.setInput(blob)\n",
    "    outputs = yolo_net.forward(output_layers)\n",
    "\n",
    "    class_ids = []\n",
    "    confidences = []\n",
    "    boxes = []\n",
    "\n",
    "    if frame_count % frame_interval == 0:\n",
    "        # Detect vehicles every 'frame_interval' frames\n",
    "        for out in outputs:\n",
    "            for detection in out:\n",
    "                scores = detection[5:]\n",
    "                class_id = np.argmax(scores)\n",
    "                confidence = scores[class_id]\n",
    "\n",
    "                if confidence > 0.5 and (classes[class_id] == 'car' or classes[class_id] == 'truck'):  # Detecting car or truck\n",
    "                    center_x = int(detection[0] * width)\n",
    "                    center_y = int(detection[1] * height)\n",
    "                    w = int(detection[2] * width)\n",
    "                    h = int(detection[3] * height)\n",
    "\n",
    "                    # Rectangle coordinates\n",
    "                    x = int(center_x - w / 2)\n",
    "                    y = int(center_y - h / 2)\n",
    "\n",
    "                    boxes.append([x, y, w, h])\n",
    "                    confidences.append(float(confidence))\n",
    "                    class_ids.append(class_id)\n",
    "\n",
    "        # Apply non-maxima suppression to remove overlapping boxes\n",
    "        indexes = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)\n",
    "\n",
    "        # Create a tracker for each detected vehicle\n",
    "        if len(indexes) > 0:\n",
    "            for i in indexes.flatten():\n",
    "                x, y, w, h = boxes[i]\n",
    "                label = str(classes[class_ids[i]])\n",
    "                confidence = confidences[i]\n",
    "\n",
    "                # Initialize tracker for each detected car or truck\n",
    "                tracker = cv2.TrackerMIL_create()\n",
    "                trackers.append(tracker)\n",
    "                tracker.init(frame, tuple(boxes[i]))\n",
    "\n",
    "                # Store information about the vehicle (class and color for tracking)\n",
    "                color = (0, 255, 0) if label == 'car' else (0, 0, 255)  # Green for car, Red for truck\n",
    "                vehicle_info.append((label, color))  # Store label and color for later use\n",
    "\n",
    "                # Draw bounding box and label for the current vehicle\n",
    "                cv2.rectangle(frame, (x, y), (x + w, y + h), color, 3)\n",
    "                cv2.putText(frame, f\"{label} ({confidence*100:.2f}%)\", (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 1, color, 2)\n",
    "\n",
    "    # Track vehicles across frames\n",
    "    for tracker, info in zip(trackers, vehicle_info):\n",
    "        success, vehicle_box = tracker.update(frame)\n",
    "        if success:\n",
    "            x, y, w, h = [int(v) for v in vehicle_box]\n",
    "\n",
    "            # Draw the bounding box for the tracked vehicle\n",
    "            label, color = info  # Get the label and color for the vehicle\n",
    "            cv2.rectangle(frame, (x, y), (x + w, y + h), color, 3)\n",
    "            cv2.putText(frame, label, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 1, color, 2)\n",
    "\n",
    "    # Show the frame with bounding boxes\n",
    "    cv2.imshow(\"Frame\", frame)\n",
    "\n",
    "    # Press 'q' to quit\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the video capture and close windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c513288-4cb2-4217-95cf-a5641022cbd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load YOLO model\n",
    "yolo_net = cv2.dnn.readNet(\"C:/Users/CG Lab/Desktop/project/project1/yolov3.weights\", \"C:/Users/CG Lab/Desktop/project/project1/yolov3.cfg\")\n",
    "layer_names = yolo_net.getLayerNames()\n",
    "output_layers = [layer_names[i - 1] for i in yolo_net.getUnconnectedOutLayers()]\n",
    "\n",
    "# Load COCO class labels\n",
    "with open(\"C:/Users/CG Lab/Desktop/project/project1/coco.names\", \"r\") as f:\n",
    "    classes = [line.strip() for line in f.readlines()]\n",
    "\n",
    "# Initialize video capture (replace 'video.mp4' with your video file)\n",
    "cap = cv2.VideoCapture(\"C:/Users/CG Lab/Desktop/project/project1/video.mp4\")\n",
    "\n",
    "# FPS (Frames per second) from the video\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "# List to store detected boxes, trackers, and their associated class ids\n",
    "trackers = []\n",
    "vehicle_info = []  # To store vehicle-related information (e.g., class, color)\n",
    "vehicle_ids = []  # To uniquely identify vehicles\n",
    "\n",
    "frame_count = 0\n",
    "frame_interval = 10  # Run detection every N frames\n",
    "\n",
    "while(cap.isOpened()):\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    frame_count += 1\n",
    "\n",
    "    # Resize the frame to speed up processing\n",
    "    frame = cv2.resize(frame, (640, 480))\n",
    "\n",
    "    # Get frame dimensions\n",
    "    height, width, channels = frame.shape\n",
    "\n",
    "    # Convert frame to blob for YOLO model input\n",
    "    blob = cv2.dnn.blobFromImage(frame, 0.00392, (416, 416), (0, 0, 0), True, crop=False)\n",
    "    yolo_net.setInput(blob)\n",
    "    outputs = yolo_net.forward(output_layers)\n",
    "\n",
    "    class_ids = []\n",
    "    confidences = []\n",
    "    boxes = []\n",
    "\n",
    "    if frame_count % frame_interval == 0:\n",
    "        # Detect vehicles every 'frame_interval' frames\n",
    "        for out in outputs:\n",
    "            for detection in out:\n",
    "                scores = detection[5:]\n",
    "                class_id = np.argmax(scores)\n",
    "                confidence = scores[class_id]\n",
    "\n",
    "                if confidence > 0.5 and (classes[class_id] == 'car' or classes[class_id] == 'truck'):  # Detecting car or truck\n",
    "                    center_x = int(detection[0] * width)\n",
    "                    center_y = int(detection[1] * height)\n",
    "                    w = int(detection[2] * width)\n",
    "                    h = int(detection[3] * height)\n",
    "\n",
    "                    # Rectangle coordinates\n",
    "                    x = int(center_x - w / 2)\n",
    "                    y = int(center_y - h / 2)\n",
    "\n",
    "                    boxes.append([x, y, w, h])\n",
    "                    confidences.append(float(confidence))\n",
    "                    class_ids.append(class_id)\n",
    "\n",
    "        # Apply non-maxima suppression to remove overlapping boxes\n",
    "        indexes = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)\n",
    "\n",
    "        # Create a tracker for each detected vehicle and prevent multiple boxes for the same object\n",
    "        if len(indexes) > 0:\n",
    "            for i in indexes.flatten():\n",
    "                x, y, w, h = boxes[i]\n",
    "                label = str(classes[class_ids[i]])\n",
    "                confidence = confidences[i]\n",
    "\n",
    "                # Only add tracker if the vehicle hasn't been tracked already\n",
    "                if label == 'car' or label == 'truck':  # Detecting car or truck\n",
    "                    vehicle_id = len(vehicle_info)  # Unique vehicle ID\n",
    "                    vehicle_ids.append(vehicle_id)\n",
    "\n",
    "                    # Initialize tracker for the detected vehicle\n",
    "                    tracker = cv2.TrackerMIL_create()\n",
    "                    trackers.append(tracker)\n",
    "                    tracker.init(frame, tuple(boxes[i]))\n",
    "\n",
    "                    # Store information about the vehicle (class and color for tracking)\n",
    "                    color = (0, 255, 0) if label == 'car' else (0, 0, 255)  # Green for car, Red for truck\n",
    "                    vehicle_info.append((label, color, x, y, w, h))  # Store label, color, and initial bounding box\n",
    "\n",
    "                    # Draw bounding box and label for the current vehicle\n",
    "                    cv2.rectangle(frame, (x, y), (x + w, y + h), color, 3)\n",
    "                    cv2.putText(frame, f\"{label} ({confidence*100:.2f}%)\", (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 1, color, 2)\n",
    "\n",
    "    # Track vehicles across frames\n",
    "    for tracker, info, vehicle_id in zip(trackers, vehicle_info, vehicle_ids):\n",
    "        success, vehicle_box = tracker.update(frame)\n",
    "        if success:\n",
    "            x, y, w, h = [int(v) for v in vehicle_box]\n",
    "\n",
    "            # Update the vehicle info with the new bounding box coordinates\n",
    "            vehicle_info[vehicle_id] = (info[0], info[1], x, y, w, h)\n",
    "\n",
    "            # Draw the bounding box for the tracked vehicle\n",
    "            label, color, _, _, _, _ = vehicle_info[vehicle_id]  # Retrieve label, color\n",
    "            cv2.rectangle(frame, (x, y), (x + w, y + h), color, 3)\n",
    "            cv2.putText(frame, label, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 1, color, 2)\n",
    "\n",
    "    # Show the frame with bounding boxes\n",
    "    cv2.imshow(\"Frame\", frame)\n",
    "\n",
    "    # Press 'q' to quit\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the video capture and close windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c136e0-9609-4fed-be6c-626f5313a8d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
